{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, BatchNormalization\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom keras.applications.vgg19 import VGG19\n\nimport cv2\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import fbeta_score\n\ninput_size = 128\ninput_channels = 3\n\nepochs = 50\nbatch_size = 128\n\nn_folds = 5\n\ntraining = True\n\nensemble_voting = False  # If True, use voting for model ensemble, otherwise use averaging\n\ndf_train_data = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\ndf_test_data = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')\n\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in df_train_data['tags'].values])))\n\nlabel_map = {l: i for i, l in enumerate(labels)}\ninv_label_map = {i: l for l, i in label_map.items()}\n\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n\nfold_count = 0\n\ny_full_test = []\nthres_sum = np.zeros(17, np.float32)\n\nfor train_index, test_index in kf.split(df_train_data):\n\n    fold_count += 1\n    print('Fold ', fold_count)\n\n\n    def transformations(src, choice):\n        if choice == 0:\n            # Rotate 90\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n        if choice == 1:\n            # Rotate 90 and flip horizontally\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n            src = cv2.flip(src, flipCode=1)\n        if choice == 2:\n            # Rotate 180\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n        if choice == 3:\n            # Rotate 180 and flip horizontally\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n            src = cv2.flip(src, flipCode=1)\n        if choice == 4:\n            # Rotate 90 counter-clockwise\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n        if choice == 5:\n            # Rotate 90 counter-clockwise and flip horizontally\n            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n            src = cv2.flip(src, flipCode=1)\n        return src\n\n    df_valid = df_train_data.ix[test_index]\n    print('Validating on {} samples'.format(len(df_valid)))\n\n\n    def valid_generator():\n        while True:\n            for start in range(0, len(df_valid), batch_size):\n                x_batch = []\n                y_batch = []\n                end = min(start + batch_size, len(df_valid))\n                df_valid_batch = df_valid[start:end]\n                for f, tags in df_valid_batch.values:\n                    img = cv2.imread('input/train-jpg/{}.jpg'.format(f))\n                    img = cv2.resize(img, (input_size, input_size))\n                    img = transformations(img, np.random.randint(6))\n                    targets = np.zeros(17)\n                    for t in tags.split(' '):\n                        targets[label_map[t]] = 1\n                    x_batch.append(img)\n                    y_batch.append(targets)\n                x_batch = np.array(x_batch, np.float32)\n                y_batch = np.array(y_batch, np.uint8)\n                yield x_batch, y_batch\n\n\n    df_train = df_train_data.ix[train_index]\n    if training:\n        print('Training on {} samples'.format(len(df_train)))\n\n\n    def train_generator():\n        while True:\n            for start in range(0, len(df_train), batch_size):\n                x_batch = []\n                y_batch = []\n                end = min(start + batch_size, len(df_train))\n                df_train_batch = df_train[start:end]\n                for f, tags in df_train_batch.values:\n                    img = cv2.imread('input/train-jpg/{}.jpg'.format(f))\n                    img = cv2.resize(img, (input_size, input_size))\n                    img = transformations(img, np.random.randint(6))\n                    targets = np.zeros(17)\n                    for t in tags.split(' '):\n                        targets[label_map[t]] = 1\n                    x_batch.append(img)\n                    y_batch.append(targets)\n                x_batch = np.array(x_batch, np.float32)\n                y_batch = np.array(y_batch, np.uint8)\n                yield x_batch, y_batch\n\n\n    base_model = VGG19(include_top=False,\n                       weights='imagenet',\n                       input_shape=(input_size, input_size, input_channels))\n\n    model = Sequential()\n    # Batchnorm input\n    model.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n    # Base model\n    model.add(base_model)\n    # Classifier\n    model.add(Flatten())\n    model.add(Dense(17, activation='sigmoid'))\n\n    opt = Adam(lr=1e-4)\n\n    model.compile(loss='binary_crossentropy',\n                  # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    callbacks = [EarlyStopping(monitor='val_loss',\n                               patience=4,\n                               verbose=1,\n                               min_delta=1e-4),\n                 ReduceLROnPlateau(monitor='val_loss',\n                                   factor=0.1,\n                                   patience=2,\n                                   cooldown=2,\n                                   verbose=1),\n                 ModelCheckpoint(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5',\n                                 save_best_only=True,\n                                 save_weights_only=True)]\n\n    if training:\n        model.fit_generator(generator=train_generator(),\n                            steps_per_epoch=(len(df_train) // batch_size) + 1,\n                            epochs=epochs,\n                            verbose=2,\n                            callbacks=callbacks,\n                            validation_data=valid_generator(),\n                            validation_steps=(len(df_valid) // batch_size) + 1)\n\n\n    def optimise_f2_thresholds(y, p, verbose=True, resolution=100):\n        def mf(x):\n            p2 = np.zeros_like(p)\n            for i in range(17):\n                p2[:, i] = (p[:, i] > x[i]).astype(np.int)\n            score = fbeta_score(y, p2, beta=2, average='samples')\n            return score\n\n        x = [0.2] * 17\n        for i in range(17):\n            best_i2 = 0\n            best_score = 0\n            for i2 in range(resolution):\n                i2 /= float(resolution)\n                x[i] = i2\n                score = mf(x)\n                if score > best_score:\n                    best_i2 = i2\n                    best_score = score\n            x[i] = best_i2\n            if verbose:\n                print(i, best_i2, best_score)\n        return x\n\n\n    # Load best weights\n    model.load_weights(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5')\n\n    p_valid = model.predict_generator(generator=valid_generator(),\n                                      steps=(len(df_valid) // batch_size) + 1)\n\n    y_valid = []\n    for f, tags in df_valid.values:\n        targets = np.zeros(17)\n        for t in tags.split(' '):\n            targets[label_map[t]] = 1\n        y_valid.append(targets)\n    y_valid = np.array(y_valid, np.uint8)\n\n    # Find optimal f2 thresholds for local validation set\n    thres = optimise_f2_thresholds(y_valid, p_valid, verbose=False)\n\n    print('F2 = {}'.format(fbeta_score(y_valid, np.array(p_valid) > thres, beta=2, average='samples')))\n\n    thres_sum += np.array(thres, np.float32)\n\n\n    def test_generator(transformation):\n        while True:\n            for start in range(0, len(df_test_data), batch_size):\n                x_batch = []\n                end = min(start + batch_size, len(df_test_data))\n                df_test_batch = df_test_data[start:end]\n                for f, tags in df_test_batch.values:\n                    img = cv2.imread('input/test-jpg/{}.jpg'.format(f))\n                    img = cv2.resize(img, (input_size, input_size))\n                    img = transformations(img, transformation)\n                    x_batch.append(img)\n                x_batch = np.array(x_batch, np.float32)\n                yield x_batch\n\n    # 6-fold TTA\n    p_full_test = []\n    for i in range(6):\n        p_test = model.predict_generator(generator=test_generator(transformation=i),\n                                         steps=(len(df_test_data) // batch_size) + 1)\n        p_full_test.append(p_test)\n\n    p_test = np.array(p_full_test[0])\n    for i in range(1, 6):\n        p_test += np.array(p_full_test[i])\n    p_test /= 6\n\n    y_full_test.append(p_test)\n\nresult = np.array(y_full_test[0])\nif ensemble_voting:\n    for f in range(len(y_full_test[0])):  # For each file\n        for tag in range(17):  # For each tag\n            preds = []\n            for fold in range(n_folds):  # For each fold\n                preds.append(y_full_test[fold][f][tag])\n            pred = Counter(preds).most_common(1)[0][0]  # Most common tag prediction among folds\n            result[f][tag] = pred\nelse:\n    for fold in range(1, n_folds):\n        result += np.array(y_full_test[fold])\n    result /= n_folds\nresult = pd.DataFrame(result, columns=labels)\n\npreds = []\nthres = (thres_sum / n_folds).tolist()\n\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.ix[[i]]\n    a = a.apply(lambda x: x > thres, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))\n\ndf_test_data['tags'] = preds\ndf_test_data.to_csv('submission.csv', index=False)","execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'input/train_v2.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-8c283a9e79a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mensemble_voting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# If True, use voting for model ensemble, otherwise use averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/train_v2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mdf_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/sample_submission_v2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input/train_v2.csv'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}